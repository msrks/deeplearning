<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Deep Learning Tips by rikipafe</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Deep Learning Tips</h1>
        <p>theory, coding, etc..</p>

        <p class="view"><a href="https://github.com/rikipafe/deeplearning">View the Project on GitHub <small>rikipafe/deeplearning</small></a></p>


        <ul>
          <li><a href="https://github.com/rikipafe/deeplearning/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/rikipafe/deeplearning/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/rikipafe/deeplearning">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="1-tensorflow--tflean" class="anchor" href="#1-tensorflow--tflean" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1) tensorflow + tflean</h1>

<h2>
<a id="launching-tensorboard" class="anchor" href="#launching-tensorboard" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Launching Tensorboard</h2>

<div class="highlight highlight-source-shell"><pre>  <span class="pl-k">&gt;</span>$ tensorboard --logdir=<span class="pl-k">&lt;</span>logdir<span class="pl-k">&gt;</span> --port=<span class="pl-k">&lt;</span>port<span class="pl-k">&gt;</span></pre></div>

<h2>
<a id="from-numpy-to-tensor" class="anchor" href="#from-numpy-to-tensor" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>from numpy to tensor</h2>

<div class="highlight highlight-source-python"><pre>  <span class="pl-c1">1</span>. tf.convert_to_tensor(np.array)
  <span class="pl-c1">2</span>. tf.placeholder</pre></div>

<h2>
<a id="fine-tuningtflearn" class="anchor" href="#fine-tuningtflearn" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Fine Tuning(TFlearn)</h2>

<p>restore=False</p>

<div class="highlight highlight-source-python"><pre>  softmax <span class="pl-k">=</span> fully_connected(network, num_classes, <span class="pl-v">restore</span><span class="pl-k">=</span><span class="pl-c1">False</span>)</pre></div>

<hr>

<h1>
<a id="2-theory" class="anchor" href="#2-theory" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2) Theory</h1>

<h2>
<a id="preprocessing" class="anchor" href="#preprocessing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>preprocessing</h2>

<p>to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature</p>

<h2>
<a id="initialization" class="anchor" href="#initialization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>initialization</h2>

<ul>
<li>xavier initialization</li>
<li>w = np.random.randn(n) * sqrt(2.0/n)</li>
<li>identity matrix initialization for RNN</li>
</ul>

<h2>
<a id="regularization" class="anchor" href="#regularization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>regularization</h2>

<ul>
<li>l2 regularization with cross validation</li>
<li>max norm constraint(gradient clipping for RNN)</li>
<li>dropout 0.5</li>
<li>use batch normalization</li>
</ul>

<h2>
<a id="small-dateset" class="anchor" href="#small-dateset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>small dateset</h2>

<ul>
<li>pretraining</li>
<li>data distortion to increse the number of datas</li>
<li>not split dataset, but use cross-validation</li>
</ul>

<h2>
<a id="model-construction-flow" class="anchor" href="#model-construction-flow" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>model construction flow</h2>

<ol>
<li>compare train vs valid accuracy</li>
<li>if not overfit: make more complicated model</li>
<li>if overfit: regularize</li>
</ol>

<h2>
<a id="loss" class="anchor" href="#loss" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>loss</h2>

<ul>
<li>l2loss is hard to optimize! donâ€™t use dropout!!</li>
<li>softmax is better</li>
</ul>

<h2>
<a id="optimizer" class="anchor" href="#optimizer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>optimizer</h2>

<h4>
<a id="sgd" class="anchor" href="#sgd" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>sgd</h4>

<h4>
<a id="momentum" class="anchor" href="#momentum" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>momentum</h4>

<p>A typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs.</p>

<h4>
<a id="nesterov-momentum" class="anchor" href="#nesterov-momentum" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Nesterov Momentum</h4>

<h4>
<a id="adagrad" class="anchor" href="#adagrad" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>adagrad</h4>

<div class="highlight highlight-source-python"><pre><span class="pl-c"># Assume the gradient dx and parameter vector x</span>
cache <span class="pl-k">+=</span> dx<span class="pl-k">**</span><span class="pl-c1">2</span>
x <span class="pl-k">+=</span> <span class="pl-k">-</span> learning_rate <span class="pl-k">*</span> dx <span class="pl-k">/</span> (np.sqrt(cache) <span class="pl-k">+</span> eps)</pre></div>

<h4>
<a id="rmsprop" class="anchor" href="#rmsprop" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>RMSprop</h4>

<div class="highlight highlight-source-python"><pre>cache <span class="pl-k">=</span> decay_rate <span class="pl-k">*</span> cache <span class="pl-k">+</span> (<span class="pl-c1">1</span> <span class="pl-k">-</span> decay_rate) <span class="pl-k">*</span> dx<span class="pl-k">**</span><span class="pl-c1">2</span>
x <span class="pl-k">+=</span> <span class="pl-k">-</span> learning_rate <span class="pl-k">*</span> dx <span class="pl-k">/</span> (np.sqrt(cache) <span class="pl-k">+</span> eps)</pre></div>

<h4>
<a id="adam" class="anchor" href="#adam" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Adam</h4>

<p>looks a bit like RMSProp with momentum</p>

<div class="highlight highlight-source-python"><pre>m <span class="pl-k">=</span> beta1<span class="pl-k">*</span>m <span class="pl-k">+</span> (<span class="pl-c1">1</span><span class="pl-k">-</span>beta1)<span class="pl-k">*</span>dx
v <span class="pl-k">=</span> beta2<span class="pl-k">*</span>v <span class="pl-k">+</span> (<span class="pl-c1">1</span><span class="pl-k">-</span>beta2)<span class="pl-k">*</span>(dx<span class="pl-k">**</span><span class="pl-c1">2</span>)
x <span class="pl-k">+=</span> <span class="pl-k">-</span> learning_rate <span class="pl-k">*</span> m <span class="pl-k">/</span> (np.sqrt(v) <span class="pl-k">+</span> eps)</pre></div>

<h2>
<a id="others" class="anchor" href="#others" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>others</h2>

<ul>
<li>model ensembles</li>
<li>annealing the learning rate
set learning rate to realize that update_scale / param_scale = ~1e-3 , when using convnet, the first layer's weight</li>
<li>step decay</li>
<li>exponential decay</li>
<li>1/t decay</li>
<li>large number of target class</li>
<li>negative sampling</li>
<li>sampled softmax</li>
</ul>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/rikipafe">rikipafe</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
