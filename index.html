<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Deep Learning Tips by rikipafe</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Deep Learning Tips</h1>
        <p>theory, coding, etc..</p>

        <p class="view"><a href="https://github.com/rikipafe/deeplearning">View the Project on GitHub <small>rikipafe/deeplearning</small></a></p>


        <ul>
          <li><a href="https://github.com/rikipafe/deeplearning/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/rikipafe/deeplearning/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/rikipafe/deeplearning">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h2>
<a id="tensorflow--tflean" class="anchor" href="#tensorflow--tflean" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>tensorflow + tflean</h2>

<p>Launching Tensorboard</p>

<div class="highlight highlight-source-shell"><pre>  <span class="pl-k">&gt;</span>$ tensorboard --logdir=<span class="pl-k">&lt;</span>logdir<span class="pl-k">&gt;</span> --port=<span class="pl-k">&lt;</span>port<span class="pl-k">&gt;</span></pre></div>

<p>from numpy to tensor</p>

<div class="highlight highlight-source-python"><pre>  <span class="pl-c1">1</span>. tf.convert_to_tensor(np.array)
  <span class="pl-c1">2</span>. tf.placeholder</pre></div>

<p>Fine Tuning
restore=False</p>

<div class="highlight highlight-source-python"><pre>  softmax <span class="pl-k">=</span> fully_connected(network, num_classes, <span class="pl-v">restore</span><span class="pl-k">=</span><span class="pl-c1">False</span>)</pre></div>

<hr>

<h2>
<a id="theory" class="anchor" href="#theory" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Theory</h2>

<p>preprocessing</p>

<ul>
<li>to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature</li>
</ul>

<p>initialization</p>

<ul>
<li>xavier initialization</li>
<li>w = np.random.randn(n) * sqrt(2.0/n)</li>
<li>identity matrix initialization for RNN</li>
</ul>

<p>regularization</p>

<ul>
<li>l2 regularization with cross validation</li>
<li>max norm constraint(gradient clipping for RNN)</li>
<li>dropout 0.5</li>
<li>use batch normalization</li>
</ul>

<p>small dateset</p>

<ul>
<li>pretraining</li>
<li>data distortion to increse the number of datas</li>
<li>not split dataset, but use cross-validation</li>
</ul>

<p>model construction flow
1. compare train vs valid accuracy
2. if not overfit: make more complicated model
3. if overfit: regularize</p>

<p>loss</p>

<ul>
<li>l2loss is hard to optimize! donâ€™t use dropout!!</li>
<li>softmax is better</li>
</ul>

<h3>
<a id="optimizer" class="anchor" href="#optimizer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>optimizer</h3>

<p>sgd</p>

<p>momentum</p>

<ul>
<li>A typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs.</li>
</ul>

<p>Nesterov Momentum</p>

<p>adagrad</p>

<div class="highlight highlight-source-python"><pre><span class="pl-c"># Assume the gradient dx and parameter vector x</span>
cache <span class="pl-k">+=</span> dx<span class="pl-k">**</span><span class="pl-c1">2</span>
x <span class="pl-k">+=</span> <span class="pl-k">-</span> learning_rate <span class="pl-k">*</span> dx <span class="pl-k">/</span> (np.sqrt(cache) <span class="pl-k">+</span> eps)</pre></div>

<p>RMSprop</p>

<div class="highlight highlight-source-python"><pre>cache <span class="pl-k">=</span> decay_rate <span class="pl-k">*</span> cache <span class="pl-k">+</span> (<span class="pl-c1">1</span> <span class="pl-k">-</span> decay_rate) <span class="pl-k">*</span> dx<span class="pl-k">**</span><span class="pl-c1">2</span>
x <span class="pl-k">+=</span> <span class="pl-k">-</span> learning_rate <span class="pl-k">*</span> dx <span class="pl-k">/</span> (np.sqrt(cache) <span class="pl-k">+</span> eps)</pre></div>

<p>Adam
looks a bit like RMSProp with momentum</p>

<div class="highlight highlight-source-python"><pre>m <span class="pl-k">=</span> beta1<span class="pl-k">*</span>m <span class="pl-k">+</span> (<span class="pl-c1">1</span><span class="pl-k">-</span>beta1)<span class="pl-k">*</span>dx
v <span class="pl-k">=</span> beta2<span class="pl-k">*</span>v <span class="pl-k">+</span> (<span class="pl-c1">1</span><span class="pl-k">-</span>beta2)<span class="pl-k">*</span>(dx<span class="pl-k">**</span><span class="pl-c1">2</span>)
x <span class="pl-k">+=</span> <span class="pl-k">-</span> learning_rate <span class="pl-k">*</span> m <span class="pl-k">/</span> (np.sqrt(v) <span class="pl-k">+</span> eps)</pre></div>

<h2>
<a id="others" class="anchor" href="#others" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>others</h2>

<p>model ensembles</p>

<p>annealing the learning rate</p>

<p>set learning rate to realize that update_scale / param_scale = ~1e-3 , when using convnet, the first layer's weight</p>

<ul>
<li>step decay</li>
<li>exponential decay</li>
<li>1/t decay</li>
</ul>

<p>large number of target class</p>

<ul>
<li>negative sampling</li>
<li>sampled softmax</li>
</ul>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/rikipafe">rikipafe</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
