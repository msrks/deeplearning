{
  "name": "Deep Learning Tips",
  "tagline": "theory, coding, etc..",
  "body": "# 1) tensorflow + tflean\r\n\r\n## Launching Tensorboard\r\n```bash\r\n  >$ tensorboard --logdir=<logdir> --port=<port>\r\n``` \r\n\r\n## from numpy to tensor\r\n```python\r\n  1. tf.convert_to_tensor(np.array)\r\n  2. tf.placeholder\r\n```\r\n\r\n## Fine Tuning(TFlearn)\r\nrestore=False\r\n```python\r\n  softmax = fully_connected(network, num_classes, restore=False)\r\n```\r\n\r\n---\r\n\r\n# 2) Theory\r\n\r\n## preprocessing\r\nto center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature\r\n\r\n## initialization\r\n- xavier initialization\r\n- w = np.random.randn(n) * sqrt(2.0/n)\r\n- identity matrix initialization for RNN\r\n\r\n## regularization\r\n- l2 regularization with cross validation\r\n- max norm constraint(gradient clipping for RNN)\r\n- dropout 0.5\r\n- use batch normalization\r\n\r\n## small dateset\r\n- pretraining\r\n- data distortion to increse the number of datas\r\n- not split dataset, but use cross-validation\r\n\r\n\r\n## model construction flow\r\n1. compare train vs valid accuracy\r\n2. if not overfit: make more complicated model\r\n3. if overfit: regularize\r\n\r\n## loss\r\n- l2loss is hard to optimize! donâ€™t use dropout!!\r\n- softmax is better\r\n\r\n## optimizer\r\n\r\n#### sgd\r\n\r\n#### momentum\r\nA typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs.\r\n\r\n#### Nesterov Momentum\r\n\r\n#### adagrad\r\n```python\r\n# Assume the gradient dx and parameter vector x\r\ncache += dx**2\r\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\r\n```\r\n\r\n#### RMSprop\r\n```python\r\ncache = decay_rate * cache + (1 - decay_rate) * dx**2\r\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\r\n```\r\n\r\n#### Adam\r\nlooks a bit like RMSProp with momentum\r\n```python\r\nm = beta1*m + (1-beta1)*dx\r\nv = beta2*v + (1-beta2)*(dx**2)\r\nx += - learning_rate * m / (np.sqrt(v) + eps)\r\n```\r\n\r\n## others\r\n\r\n* model ensembles\r\n* annealing the learning rate\r\nset learning rate to realize that update_scale / param_scale = ~1e-3 , when using convnet, the first layer's weight\r\n- step decay\r\n- exponential decay\r\n- 1/t decay\r\n* large number of target class\r\n- negative sampling\r\n- sampled softmax\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}