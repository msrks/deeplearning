{
  "name": "Deep Learning Tips",
  "tagline": "theory, coding, etc..",
  "body": "## TensorFlow + tflean\r\n\r\nLaunching Tensorboard\r\n```bash\r\n$ tensorboard --logdir=<logdir> --port=<port>\r\n``` \r\n\r\nfrom numpy to tensor\r\n```python\r\n1. tf.convert_to_tensor(np.array)\r\n2. tf.placeholder\r\n```\r\n\r\nFine Tuning\r\nrestore=False\r\n```python\r\nsoftmax = fully_connected(network, num_classes, restore=False)\r\n```\r\n\r\n---\r\n## TensorFlow(Gpu Ver.) Installation @160507\r\n\r\n1) cuda 7.5\r\n```bash\r\n$ sudo apt-get install nvidia-361\r\n$ sudo apt-get install nvidia-cuds-toolkit\r\n$ sudo reboot\r\n```\r\n\r\n2) cuDNN 4\r\n```bash\r\n# download .whl @firefox (directly access the follwing link)\r\n# https://developer.nvidia.com/rdp/form/cudnn-download-survey\r\n$ tar xzf cudnn-7.0-linux-x64-v4.0-prod.tgz\r\n$ sudo cp -a cuda/lib64/* /usr/local/lib/\r\n$ sudo cp -a cuda/include/* /usr/local/include/\r\n$ sudo ldconfig\r\n```\r\n\r\n3)tensorflow\r\n```bash\r\n$ sudo apt-get install python-dev python-virtualenv\r\n# download .whl @firefox (directly access the follwing link)\r\n# https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\r\n$ cd Download\r\n$ sudo pip install tensorflow-0.8.0-cp27-none-linux_x86_64.whl\r\n```\r\n\r\nreference\r\n> http://qiita.com/yukoba/items/3692f1cb677b2383c983\r\n> http://qiita.com/akiraak/items/1c7fb452fb7721292071#cudnn-v4-%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB\r\n> https://www.tensorflow.org/versions/master/get_started/os_setup.html\r\n\r\n---\r\n## Theory\r\n\r\npreprocessing\r\n- to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature\r\n\r\ninitialization\r\n- xavier initialization\r\n- w = np.random.randn(n) * sqrt(2.0/n)\r\n- identity matrix initialization for RNN\r\n\r\nregularization\r\n- l2 regularization with cross validation\r\n- max norm constraint(gradient clipping for RNN)\r\n- dropout 0.5\r\n- use batch normalization\r\n\r\nsmall dateset\r\n- pretraining\r\n- data distortion to increse the number of datas\r\n- not split dataset, but use cross-validation\r\n\r\nmodel construction flow\r\n1. compare train vs valid accuracy\r\n2. if not overfit: make more complicated model\r\n3. if overfit: regularize\r\n\r\nloss\r\n- l2loss is hard to optimize! donâ€™t use dropout!!\r\n- softmax is better\r\n\r\n### optimizer\r\n\r\nsgd\r\n\r\nmomentum\r\n* A typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs.\r\n\r\nNesterov Momentum\r\n\r\nadagrad\r\n```python\r\n# Assume the gradient dx and parameter vector x\r\ncache += dx**2\r\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\r\n```\r\n\r\nRMSprop\r\n```python\r\ncache = decay_rate * cache + (1 - decay_rate) * dx**2\r\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\r\n```\r\n\r\nAdam\r\nlooks a bit like RMSProp with momentum\r\n```python\r\nm = beta1*m + (1-beta1)*dx\r\nv = beta2*v + (1-beta2)*(dx**2)\r\nx += - learning_rate * m / (np.sqrt(v) + eps)\r\n```\r\n\r\n## others\r\nmodel ensembles\r\n\r\nannealing the learning rate\r\n\r\nset learning rate to realize that update_scale / param_scale = ~1e-3 , when using convnet, the first layer's weight\r\n- step decay\r\n- exponential decay\r\n- 1/t decay\r\n\r\nlarge number of target class\r\n- negative sampling\r\n- sampled softmax",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}